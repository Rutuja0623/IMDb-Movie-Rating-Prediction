{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Score prediction for movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset under consideration contains information about various movies. It encompasses details such as directors, cast members (actors), critic reviews, audience reactions, and other related attributes. Among the crucial metrics used to gauge a movie's success, the IMDb score holds a prominent position. The IMDb score is a numerical representation of how well-received a movie is among the general audience and critics alike. It reflects the average rating given by viewers and provides insights into a movie's popularity and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Data \n",
    "\n",
    "movie_IMDB_df=pd.read_csv(\"movie_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " movie_IMDB_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics for numerical columns, represented by five key points \n",
    "\n",
    "movie_IMDB_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the color section as most of the movies is colored\n",
    "\n",
    "movie_IMDB_df[\"color\"].value_counts()\n",
    "\n",
    "movie_IMDB_df.drop('color',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the Imdb link from the dataset\n",
    "\n",
    "movie_IMDB_df.drop('movie_imdb_link', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the columns present in the datset\n",
    "\n",
    "movie_IMDB_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking No of the missing values in the dataset\n",
    "\n",
    "movie_IMDB_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate dataset null values with low counts to retain more data effectively\n",
    "\n",
    "movie_IMDB_df.dropna(axis=0,subset=['director_name', 'num_critic_for_reviews','duration','director_facebook_likes','actor_3_facebook_likes','actor_2_name','actor_1_facebook_likes','actor_1_name','actor_3_name','facenumber_in_poster','num_user_for_reviews','language','country','actor_2_facebook_likes','plot_keywords'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the content rating with Value R as it has highest frequency\n",
    "\n",
    "movie_IMDB_df[\"content_rating\"].fillna(\"R\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the aspect_ratio with the median of the value as the graph is right skewed \n",
    "\n",
    "movie_IMDB_df[\"aspect_ratio\"].fillna(movie_IMDB_df[\"aspect_ratio\"].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the value in budget with the median of the value\n",
    "\n",
    "movie_IMDB_df[\"budget\"].fillna(movie_IMDB_df[\"budget\"].median(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the value in gross with the median of the value \n",
    "\n",
    "movie_IMDB_df['gross'].fillna(movie_IMDB_df['gross'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck that all the null values are removed\n",
    "\n",
    "movie_IMDB_df.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the duplicate values in the dataset\n",
    "\n",
    "movie_IMDB_df.drop_duplicates(inplace=True)\n",
    "movie_IMDB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of the language values \n",
    "\n",
    "movie_IMDB_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most of the values for the languages is english therefore drop the english column\n",
    "\n",
    "movie_IMDB_df.drop('language',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column to check the net profit made by the company  \n",
    "\n",
    "movie_IMDB_df[\"Profit\"]=movie_IMDB_df['budget'].sub(movie_IMDB_df['gross'], axis = 0) \n",
    "\n",
    "movie_IMDB_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column to check the profit percentage made by the company \n",
    "\n",
    "movie_IMDB_df['Profit_Percentage']=(movie_IMDB_df[\"Profit\"]/movie_IMDB_df[\"gross\"])*100\n",
    "movie_IMDB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts for the countries \n",
    "\n",
    "value_counts=movie_IMDB_df[\"country\"].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting top 2 values of index\n",
    "\n",
    "vals = value_counts[:2].index\n",
    "print (vals)\n",
    "movie_IMDB_df['country'] = movie_IMDB_df.country.where(movie_IMDB_df.country.isin(vals), 'other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divided the country into three catogories \n",
    "movie_IMDB_df[\"country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data Visualization and EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Profit_Percentage' is a calculated column in your movie_df\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=movie_IMDB_df, x='imdb_score', y='Profit_Percentage')\n",
    "plt.title('Relationship between IMDb Score and Profit Percentage')\n",
    "plt.xlabel('IMDb Score')\n",
    "plt.ylabel('Profit Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the corelation between imdb_rating with respect to Critic Reviews \n",
    "\n",
    "(ggplot(movie_IMDB_df)\n",
    " + aes(x='imdb_score', y='num_critic_for_reviews')\n",
    " + geom_line()\n",
    " + labs(title='IMDB_Score vs. Critic Reviews', x='IMDB scores', y='Critic Reviews')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 movies based on the profit they made\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='Profit', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.pointplot(data=movie_IMDB_df_new, x='Profit', y='budget', hue='movie_title')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 movies based on the profit percentage they made\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='Profit_Percentage', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.pointplot(data=movie_IMDB_df_new, x='Profit_Percentage', y='budget', hue='movie_title')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.xlabel('Profit Percentage')\n",
    "plt.ylabel('Budget')\n",
    "plt.title('Top 20 Movies Based on Profit Percentage')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commercial success vs critial acclaim\n",
    "\n",
    "movie_IMDB_df= movie_IMDB_df.sort_values(by ='Profit_Percentage' , ascending=False)\n",
    "movie_IMDB_df_new=movie_IMDB_df.head(20)\n",
    "(ggplot(movie_IMDB_df_new)\n",
    " + aes(x='imdb_score', y='gross',color = \"content_rating\")\n",
    " + geom_point()\n",
    " +  geom_hline(aes(yintercept = 600)) + \n",
    "  geom_vline(aes(xintercept = 10)) + \n",
    "  xlab(\"Imdb score\") + \n",
    "  ylab(\"Gross money earned in million dollars\") + \n",
    "  ggtitle(\"Commercial success Vs Critical acclaim\") +\n",
    "  annotate(\"text\", x = 8.5, y = 700, label = \"High ratings \\n & High gross\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 actors of movies based on the commerical success\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='Profit_Percentage', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.pointplot(x='actor_1_name', y='Profit_Percentage', data=movie_IMDB_df_new, hue='movie_title')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 actors of movies based on the commerical success\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='Profit_Percentage', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.barplot(x='Profit_Percentage', y='actor_1_name', data=movie_IMDB_df_new, hue='movie_title', dodge=False)\n",
    "ax.set_xlabel('Profit Percentage')\n",
    "ax.set_ylabel('Actor')\n",
    "ax.set_title('Top 20 Actors Based on Commercial Success')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 actors of movies based on the imdb rating of the movies \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='imdb_score', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.pointplot(x='actor_1_name', y='imdb_score', data=movie_IMDB_df_new, hue='movie_title')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country of Top 20 movies based on imdb rating\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "movie_IMDB_df = movie_IMDB_df.sort_values(by='imdb_score', ascending=False)\n",
    "movie_IMDB_df_new = movie_IMDB_df.head(20)\n",
    "ax = sns.barplot(x='country', y='imdb_score', data=movie_IMDB_df_new, hue='movie_title')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Data Preparation for the models - Dropping the columns with categorical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('director_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('actor_1_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('actor_2_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('actor_3_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('movie_title',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('plot_keywords',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('genres',axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('Profit',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('Profit_Percentage',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with heat map\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "corr = movie_IMDB_df.corr()\n",
    "sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\n",
    "plt.figure(figsize=(13,7))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, 1)] = True\n",
    "a = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\n",
    "rotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\n",
    "roty = a.set_yticklabels(a.get_yticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the facebook likes of actor 2 and actor 3 together \n",
    "movie_IMDB_df['Other_actor_facebbok_likes']=movie_IMDB_df[\"actor_2_facebook_likes\"] + movie_IMDB_df['actor_3_facebook_likes']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the actor 2 and actor 3 facebook likes columns as they have been added together \n",
    "\n",
    "movie_IMDB_df.drop('actor_2_facebook_likes',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('actor_3_facebook_likes',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.drop('cast_total_facebook_likes',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of the ratio of num_user_for_reviews and num_critic_for_reviews.\n",
    "\n",
    "movie_IMDB_df['critic_review_ratio']=movie_IMDB_df['num_critic_for_reviews']/movie_IMDB_df['num_user_for_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the num_critic_for_review\n",
    "\n",
    "movie_IMDB_df.drop('num_critic_for_reviews',axis=1,inplace=True)\n",
    "movie_IMDB_df.drop('num_user_for_reviews',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Correlation matrix shown in the figure \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "corr = movie_IMDB_df.corr()\n",
    "sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\n",
    "plt.figure(figsize=(13,7))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, 1)] = True\n",
    "a = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\n",
    "rotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\n",
    "roty = a.set_yticklabels(a.get_yticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see none of the attributes are not much correlated to each other.All are below 0.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to categorize the imdb values in the range of 0-4,4-6,6-8 and 8-10 to mark them as the bad,average,good and excellent movies respectively\n",
    "\n",
    "movie_IMDB_df[\"imdb_binned_score\"]=pd.cut(movie_IMDB_df['imdb_score'], bins=[0,4,6,8,10], right=True, labels=False)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the imdb_score column as it is being replaced with the imdb_binned_score values \n",
    "movie_IMDB_df.drop('imdb_score',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Handling the categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df = pd.get_dummies(data = movie_IMDB_df, columns = ['country'] , prefix = ['country'] , drop_first = True)\n",
    "movie_IMDB_df = pd.get_dummies(data = movie_IMDB_df, columns = ['content_rating'] , prefix = ['content_rating'] , drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_IMDB_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Splitting the data into training and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and test data\n",
    "X=pd.DataFrame(columns=['duration','director_facebook_likes','actor_1_facebook_likes','gross','num_voted_users','facenumber_in_poster','budget','title_year','aspect_ratio','movie_facebook_likes','Other_actor_facebbok_likes','critic_review_ratio','country_USA','country_other','content_rating_G','content_rating_GP','content_rating_M','content_rating_NC-17','content_rating_Not Rated','content_rating_PG','content_rating_PG-13','content_rating_Passed','content_rating_R','content_rating_TV-14','content_rating_TV-G','content_rating_TV-PG','content_rating_Unrated','content_rating_X'],data=movie_IMDB_df)\n",
    "y=pd.DataFrame(columns=['imdb_binned_score'],data=movie_IMDB_df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.Feature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, np.ravel(y_train, order='C'))\n",
    "best_svc = grid_search.best_estimator_\n",
    "svcpred = best_svc.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, svcpred)\n",
    "print(cnf_matrix)\n",
    "print(\"Accuracy for SVC:\", metrics.accuracy_score(y_test, svcpred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, np.ravel(y_train, order='C'))\n",
    "best_rfc = grid_search.best_estimator_\n",
    "\n",
    "rfcpred = best_rfc.predict(X_test)\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, rfcpred)\n",
    "print(cnf_matrix)\n",
    "print(\"Accuracy for Random Forest:\", metrics.accuracy_score(y_test, rfcpred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gbcl = GradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(estimator=gbcl, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, np.ravel(y_train, order='C'))\n",
    "best_gbcl = grid_search.best_estimator_\n",
    "\n",
    "test_pred = best_gbcl.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, test_pred)\n",
    "print(cnf_matrix)\n",
    "print(\"Accuracy for Gradient Boosting:\", metrics.accuracy_score(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Model comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('SVC Reports\\n',classification_report(y_test, svcpred))\n",
    "print('Random Forests Reports\\n',classification_report(y_test, rfcpred))\n",
    "print('Gradient Boosting',classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the accuracy scores for each model\n",
    "svc_accuracy = 0.72  \n",
    "rfc_accuracy = 0.75  \n",
    "gbcl_accuracy = 0.74 \n",
    "\n",
    "# Create a new plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "model_names = ['SVC', 'Random Forests', 'Gradient Boosting']\n",
    "accuracies = [svc_accuracy, rfc_accuracy, gbcl_accuracy]\n",
    "\n",
    "\n",
    "for model_name, accuracy in zip(model_names, accuracies):\n",
    "    plt.bar(model_name, accuracy)\n",
    "\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.yticks([i / 10 for i in range(11)])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Classification Reports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "svc_report = classification_report(y_test, svcpred, output_dict=True)\n",
    "rfc_report = classification_report(y_test, rfcpred, output_dict=True)\n",
    "gbcl_report = classification_report(y_test, test_pred, output_dict=True)\n",
    "\n",
    "\n",
    "reports = {\n",
    "    'SVC': svc_report,\n",
    "    'Random Forests': rfc_report,\n",
    "    'Gradient Boosting': gbcl_report\n",
    "}\n",
    "\n",
    "model_names = ['SVC', 'Random Forests', 'Gradient Boosting']\n",
    "metric_names = ['precision', 'recall', 'f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 15))\n",
    "\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    ax = axes[i]\n",
    "    ax.set_title(metric_name.capitalize())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        score = reports[model_name]['weighted avg'][metric_name]\n",
    "        ax.bar(model_name, score)\n",
    "\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_yticks([i / 10 for i in range(11)])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
